

# 客户端将请求加入待发送队列

客户端发送请求到leader处理后，会通过zab协议同步给follower，以create举例

核心流程：

1. 客户端将请求封装为Packet后，放入outgoingQueue发送队列
2. 客户端执行OP_WRITE事件，将outgoingQueue队列数据发送给服务端
3. 客户端阻塞在消息Packet的finished上，等待接收到服务端返回的响应
4. leader接收到请求后，PrepRequestProcessor开启事物消息根据请求类型处理，并交给下一个处理器
5. ProposalRequestProcessor将请求封装为Proposal类型交给下一个CommitProcessor阻塞等待commit的完成
6. 然后ProposalRequestProcessor将Proposal发送给所有follower去进行各自的刷盘操作
7. leader通过SyncRequestProcessor将Proposal刷盘并交给AckRequestProcessor直接返回ACK
8. follower接收到leader发送的Proposal消息后，通过SyncRequestProcessor追加到本地磁盘
9. 然后follower通过SendAckRequestProcessor处理器发送消息对应的ACK给leader
10. 当leader接收到大多数follower的ack响应后，再次发送COMMIT消息给所有follower
11. 然后leader再次调用CommitProcessor处理器完成自己的commit操作，将事物消息作用于内存数据库，并交给后续处理器返回响应给客户端
12. follower接收到COMMIT后操作内存数据库，也同样交给后续处理器返回响应

假如客户端执行的是create创建节点。创建path对应的节点，并且设置值为data，CreateMode标识节点是否是临时节点，或者顺序节点。临时节点，将会在客户端session失效后被删除。顺序节点，将会在path名称后拼接上一个10位补0的递增数字

如果path节点已经存在会抛出KeeperException异常，创建临时的节点将永远不会抛出这个异常。如果是创建的path对应父节点不存在，则会抛出KeeperException.NoNode。临时节点不允许有子节点存在，如果创建则抛出KeeperException.NoChildren异常。节点数据值最大允许1MB，超过将抛出KeeperExecption异常

```java
public String create(final String path, byte data[], List<ACL> acl,CreateMode createMode) throws KeeperException, InterruptedException
{
    final String clientPath = path;
    PathUtils.validatePath(clientPath, createMode.isSequential());

    final String serverPath = prependChroot(clientPath);

    //创建create请求类型
    RequestHeader h = new RequestHeader();
    h.setType(ZooDefs.OpCode.create);
    CreateRequest request = new CreateRequest();
    CreateResponse response = new CreateResponse();
    request.setData(data);
    //节点类型封装为了toFlag
    request.setFlags(createMode.toFlag());
    request.setPath(serverPath);
    if (acl != null && acl.size() == 0) {
        throw new KeeperException.InvalidACLException();
    }
    request.setAcl(acl);
    //交给客户端ClientCnxn处理请求
    ReplyHeader r = cnxn.submitRequest(h, request, response, null);
    if (r.getErr() != 0) {
        throw KeeperException.create(KeeperException.Code.get(r.getErr()),
                                     clientPath);
    }
    if (cnxn.chrootPath == null) {
        return response.getPath();
    } else {
        return response.getPath().substring(cnxn.chrootPath.length());
    }
}
```

封装请求为packet放入待发送队列并阻塞等待响应。请求交给的是ClientCnxn进行网络IO处理

````java
public ReplyHeader submitRequest(RequestHeader h, Record request,
                                 Record response, WatchRegistration watchRegistration)
    throws InterruptedException {
    ReplyHeader r = new ReplyHeader();
    //将请求封装为Packet
    Packet packet = queuePacket(h, r, request, response, null, null, null,
                                null, watchRegistration);
    synchronized (packet) {
        //阻塞等待响应结果
        //客户端接收到响应后会将finished设置为true，并且唤醒这里的阻塞
        while (!packet.finished) {
            packet.wait();
        }
    }
    return r;
}
````

封装请求为packet并加入待发送队列，同时唤醒客户端执行NIO的select

```java
Packet queuePacket(RequestHeader h, ReplyHeader r, Record request,
                   Record response, AsyncCallback cb, String clientPath,
                   String serverPath, Object ctx, WatchRegistration watchRegistration)
{
    Packet packet = null;

    // Note that we do not generate the Xid for the packet yet. It is
    // generated later at send-time, by an implementation of ClientCnxnSocket::doIO(),
    // where the packet is actually sent.
    synchronized (outgoingQueue) {
        packet = new Packet(h, r, request, response, watchRegistration);
        packet.cb = cb;
        packet.ctx = ctx;
        packet.clientPath = clientPath;
        packet.serverPath = serverPath;
        if (!state.isAlive() || closing) {
            conLossPacket(packet);
        } else {
            // If the client is asking to close the session then
            // mark as closing
            if (h.getType() == OpCode.closeSession) {
                closing = true;
            }
            //将请求放入待发送队列
            outgoingQueue.add(packet);
        }
    }
    //唤醒socketChannel注册的selector执行select操作
    sendThread.getClientCnxnSocket().wakeupCnxn();
    return packet;
}
```

# 客户端通过ClientCnxn执行OP_WRITE事件

唤醒的当前select好像并不会关注OP_WRITE马上执行，而是在当前select完后，会判断outgoingQueue队列有数据并关注OP_WRITE，下一次再执行select才会真正执行OP_WRITE的逻辑？

ClientCnxn线程不停地循环执行IO操作，因为outgoingQueue队列有待发送数据存在，所以会执行OP_WRITE事件。数据发送出去后，会加入到pendingQueue队列中

每次执行完OP_WRITE事件后，判断是否需要继续关注OP_WRITE，避免下一次的无效执行

```java
//处理写IO
if (sockKey.isWritable()) {
    synchronized(outgoingQueue) {
        //获取队列需要发送的数据包
        Packet p = findSendablePacket(outgoingQueue,
                                      cnxn.sendThread.clientTunneledAuthenticationInProgress());

        if (p != null) {
            //更新最近发送时间
            updateLastSend();
            // If we already started writing p, p.bb will already exist
            if (p.bb == null) {
                if ((p.requestHeader != null) &&
                    (p.requestHeader.getType() != OpCode.ping) &&
                    (p.requestHeader.getType() != OpCode.auth)) {
                    p.requestHeader.setXid(cnxn.getXid());
                }
                //序列化需要发送的数据，并且添加header长度
                p.createBB();
            }
            //将数据通过socket发送出去
            //如果发生粘包拆包继续发送
            sock.write(p.bb);
            if (!p.bb.hasRemaining()) {
                //如果发送完成，则从outgoingQueue队列移除，并且添加到pendingQueue队列中
                sentCount++;
                outgoingQueue.removeFirstOccurrence(p);
                //connectRequest请求的requestHeader是null，因此不会加入到pendingQueue队列
                //ping、auth类型的请求都不会加入到pendingQueue队列
                if (p.requestHeader != null
                    && p.requestHeader.getType() != OpCode.ping
                    && p.requestHeader.getType() != OpCode.auth) {
                    synchronized (pendingQueue) {
                        pendingQueue.add(p);
                    }
                }
            }
        }
        //如果outgoingQueue还有数据包需要发送则继续关注OP_WRITE事件
        if (outgoingQueue.isEmpty()) {
            // No more packets to send: turn off write interest flag.
            // Will be turned on later by a later call to enableWrite(),
            // from within ZooKeeperSaslClient (if client is configured
            // to attempt SASL authentication), or in either doIO() or
            // in doTransport() if not.
            disableWrite();
        } else {
            // Just in case
            enableWrite();
        }
    }
}
```

# Leader接收数据链式处理

在服务端NIOServerCnxnFactory线程执行IO时，发现客户端有数据发送过来，执行OP_READ事件，请求解析完后执行readRequest方法

```java
private void readRequest() throws IOException {
    zkServer.processPacket(this, incomingBuffer);
}
```

请求序列化后通过submitRequest处理

```java
public void processPacket(ServerCnxn cnxn, ByteBuffer incomingBuffer) throws IOException {
    ...
    //非权限验证的请求也是通过submitRequest处理
    Request si = new Request(cnxn, cnxn.getSessionId(), h.getXid(),h.getType(), incomingBuffer, cnxn.getAuthInfo());
    si.setOwner(ServerCnxn.me);
    //请求链式处理
    submitRequest(si);
    ...
}
```

数据链式处理逻辑

```java
/**
 * 处理客户端发送过来的所有请求（排除ping auth特殊类型）
 * @param si
 */
public void submitRequest(Request si) {
    if (firstProcessor == null) {
        synchronized (this) {
            try {
                while (!running) {
                    wait(1000);
                }
            } catch (InterruptedException e) {
                LOG.warn("Unexpected interruption", e);
            }
            if (firstProcessor == null) {
                throw new RuntimeException("Not started");
            }
        }
    }
    try {
        //重新更新session有效时间，不管是不是客户端发送的ping请求
        touch(si.cnxn);
        boolean validpacket = Request.isValid(si.type);
        if (validpacket) {
            //交给责任链处理消息数据
            firstProcessor.processRequest(si);
            if (si.cnxn != null) {
                incInProcess();
            }
        } else {
            LOG.warn("Dropping packet at server of type " + si.type);
            // if invalid packet drop the packet.
        }
    } catch (MissingSessionException e) {
        if (LOG.isDebugEnabled()) {
            LOG.debug("Dropping request: " + e.getMessage());
        }
    } catch (RequestProcessorException e) {
        LOG.error("Unable to process request:" + e.getMessage(), e);
    }
}
```

leader的处理链路为

```java
@Override
//不同的server角色，处理链路不同
protected void setupRequestProcessors() {
    RequestProcessor finalProcessor = new FinalRequestProcessor(this);
    RequestProcessor toBeAppliedProcessor = new Leader.ToBeAppliedRequestProcessor(
        finalProcessor, getLeader().toBeApplied);
    commitProcessor = new CommitProcessor(toBeAppliedProcessor,
                                          Long.toString(getServerId()), false);
    commitProcessor.start();
    ProposalRequestProcessor proposalProcessor = new ProposalRequestProcessor(this,
                                                                              commitProcessor);
    proposalProcessor.initialize();
    firstProcessor = new PrepRequestProcessor(this, proposalProcessor);
    ((PrepRequestProcessor)firstProcessor).start();
}
```

Leader的第一个处理器firstProcessor是PrepRequestProcessor，跟Follower是不一样的。需要注意的是，数据链的处理都是处理的客户端通过NIO发送的请求，Leader与Follower之间的通信是基于之前建立好的bio

## PrepRequestProcessor开启事物消息

将请求加入处理队列中，本身作为线程来根据不同的请求类型处理不同的逻辑，并且顺序性的给每个请求赋值zxid

```java
public void processRequest(Request request) {
    // request.addRQRec(">prep="+zks.outstandingChanges.size());
    submittedRequests.add(request);
}
```

从队列中获取请求根据请求类型执行不同的逻辑

```java
@Override
public void run() {
    try {
        while (true) {
            //从队列获取数据
            Request request = submittedRequests.take();
            long traceMask = ZooTrace.CLIENT_REQUEST_TRACE_MASK;
            if (request.type == OpCode.ping) {
                traceMask = ZooTrace.CLIENT_PING_TRACE_MASK;
            }
            if (LOG.isTraceEnabled()) {
                ZooTrace.logRequest(LOG, traceMask, 'P', request, "");
            }
            if (Request.requestOfDeath == request) {
                break;
            }
            //根据请求类型执行不同处理逻辑
            pRequest(request);
        }
    } catch (InterruptedException e) {
        LOG.error("Unexpected interruption", e);
    } catch (RequestProcessorException e) {
        if (e.getCause() instanceof XidRolloverException) {
            LOG.info(e.getCause().getMessage());
        }
        LOG.error("Unexpected exception", e);
    } catch (Exception e) {
        LOG.error("Unexpected exception", e);
    }
    LOG.info("PrepRequestProcessor exited loop!");
}
```

根据不同的请求封装后移交给pRequest2Txn来处理

```java
/**
 * This method will be called inside the ProcessRequestThread, which is a
 * singleton, so there will be a single thread calling this code.
 *
 * @param request
 */
@SuppressWarnings("unchecked")
protected void pRequest(Request request) throws RequestProcessorException {
    // LOG.info("Prep>>> cxid = " + request.cxid + " type = " +
    // request.type + " id = 0x" + Long.toHexString(request.sessionId));
    request.hdr = null;
    request.txn = null;

    try {
        switch (request.type) {
            //处理客户端create请求
            case OpCode.create:
                CreateRequest createRequest = new CreateRequest();
                pRequest2Txn(request.type, zks.getNextZxid(), request, createRequest, true);
                break;
            case OpCode.delete:
                DeleteRequest deleteRequest = new DeleteRequest();               
                pRequest2Txn(request.type, zks.getNextZxid(), request, deleteRequest, true);
                break;
            case OpCode.setData:
                SetDataRequest setDataRequest = new SetDataRequest();                
                pRequest2Txn(request.type, zks.getNextZxid(), request, setDataRequest, true);
                break;
            case OpCode.setACL:
                SetACLRequest setAclRequest = new SetACLRequest();                
                pRequest2Txn(request.type, zks.getNextZxid(), request, setAclRequest, true);
                break;
            case OpCode.check:
                CheckVersionRequest checkRequest = new CheckVersionRequest();              
                pRequest2Txn(request.type, zks.getNextZxid(), request, checkRequest, true);
                break;
            case OpCode.multi:
                MultiTransactionRecord multiRequest = new MultiTransactionRecord();
                try {
                    ByteBufferInputStream.byteBuffer2Record(request.request, multiRequest);
                } catch(IOException e) {
                    request.hdr =  new TxnHeader(request.sessionId, request.cxid, zks.getNextZxid(),
                                                 zks.getTime(), OpCode.multi);
                    throw e;
                }
                List<Txn> txns = new ArrayList<Txn>();
                //Each op in a multi-op must have the same zxid!
                long zxid = zks.getNextZxid();
                KeeperException ke = null;

                //Store off current pending change records in case we need to rollback
                HashMap<String, ChangeRecord> pendingChanges = getPendingChanges(multiRequest);

                int index = 0;
                for(Op op: multiRequest) {
                    Record subrequest = op.toRequestRecord() ;

                    /* If we've already failed one of the ops, don't bother
                     * trying the rest as we know it's going to fail and it
                     * would be confusing in the logfiles.
                     */
                    if (ke != null) {
                        request.hdr.setType(OpCode.error);
                        request.txn = new ErrorTxn(Code.RUNTIMEINCONSISTENCY.intValue());
                    } 

                    /* Prep the request and convert to a Txn */
                    else {
                        try {
                            pRequest2Txn(op.getType(), zxid, request, subrequest, false);
                        } catch (KeeperException e) {
                            if (ke == null) {
                                ke = e;
                            }
                            request.hdr.setType(OpCode.error);
                            request.txn = new ErrorTxn(e.code().intValue());
                            LOG.info("Got user-level KeeperException when processing "
                                     + request.toString() + " aborting remaining multi ops."
                                     + " Error Path:" + e.getPath()
                                     + " Error:" + e.getMessage());

                            request.setException(e);

                            /* Rollback change records from failed multi-op */
                            rollbackPendingChanges(zxid, pendingChanges);
                        }
                    }

                    //FIXME: I don't want to have to serialize it here and then
                    //       immediately deserialize in next processor. But I'm 
                    //       not sure how else to get the txn stored into our list.
                    ByteArrayOutputStream baos = new ByteArrayOutputStream();
                    BinaryOutputArchive boa = BinaryOutputArchive.getArchive(baos);
                    request.txn.serialize(boa, "request") ;
                    ByteBuffer bb = ByteBuffer.wrap(baos.toByteArray());

                    txns.add(new Txn(request.hdr.getType(), bb.array()));
                    index++;
                }

                request.hdr = new TxnHeader(request.sessionId, request.cxid, zxid, zks.getTime(), request.type);
                request.txn = new MultiTxn(txns);

                break;

                //create/close session don't require request record
            case OpCode.createSession:
            case OpCode.closeSession:
                pRequest2Txn(request.type, zks.getNextZxid(), request, null, true);
                break;

                //All the rest don't need to create a Txn - just verify session
            case OpCode.sync:
            case OpCode.exists:
            case OpCode.getData:
            case OpCode.getACL:
            case OpCode.getChildren:
            case OpCode.getChildren2:
            case OpCode.ping:
            case OpCode.setWatches:
                zks.sessionTracker.checkSession(request.sessionId,
                                                request.getOwner());
                break;
        }
    } catch (KeeperException e) {
        if (request.hdr != null) {
            request.hdr.setType(OpCode.error);
            request.txn = new ErrorTxn(e.code().intValue());
        }
        LOG.info("Got user-level KeeperException when processing "
                 + request.toString()
                 + " Error Path:" + e.getPath()
                 + " Error:" + e.getMessage());
        request.setException(e);
    } catch (Exception e) {
        // log at error level as we are returning a marshalling
        // error to the user
        LOG.error("Failed to process " + request, e);

        StringBuilder sb = new StringBuilder();
        ByteBuffer bb = request.request;
        if(bb != null){
            bb.rewind();
            while (bb.hasRemaining()) {
                sb.append(Integer.toHexString(bb.get() & 0xff));
            }
        } else {
            sb.append("request buffer is null");
        }

        LOG.error("Dumping request buffer: 0x" + sb.toString());
        if (request.hdr != null) {
            request.hdr.setType(OpCode.error);
            request.txn = new ErrorTxn(Code.MARSHALLINGERROR.intValue());
        }
    }
    //消息的zxid是简单递增生成的
    request.zxid = zks.getZxid();
    //将请求交给下一个处理器处理
    nextProcessor.processRequest(request);
}
```

不同类型的操作，最终都将封装为节点数据结构ChangeRecord，放入outstandingChanges队列维护，或者将节点path与ChangeRecord放入到map的outstandingChangesForPath结构中维护

outstandingChanges 位于ZooKeeperServer 中，用于存放刚进行更改还没有同步到ZKDatabase中的节点信息

```java
/**
* This method will be called inside the ProcessRequestThread, which is a
* singleton, so there will be a single thread calling this code.
*
* @param type
* @param zxid
* @param request
* @param record
*/
@SuppressWarnings("unchecked")
protected void pRequest2Txn(int type, long zxid, Request request, Record record, boolean deserialize)
    throws KeeperException, IOException, RequestProcessorException
{
    request.hdr = new TxnHeader(request.sessionId, request.cxid, zxid,
                                zks.getTime(), type);

    switch (type) {
        case OpCode.create:
            //校验session有效性
            zks.sessionTracker.checkSession(request.sessionId, request.getOwner());
            CreateRequest createRequest = (CreateRequest)record;   
            if(deserialize)
                ByteBufferInputStream.byteBuffer2Record(request.request, createRequest);
            String path = createRequest.getPath();
            //请求path路径必需包含/
            int lastSlash = path.lastIndexOf('/');
            if (lastSlash == -1 || path.indexOf('\0') != -1 || failCreate) {
                LOG.info("Invalid path " + path + " with session 0x" +
                         Long.toHexString(request.sessionId));
                throw new KeeperException.BadArgumentsException(path);
            }
            List<ACL> listACL = removeDuplicates(createRequest.getAcl());
            if (!fixupACL(request.authInfo, listACL)) {
                throw new KeeperException.InvalidACLException(path);
            }
            String parentPath = path.substring(0, lastSlash);
            ChangeRecord parentRecord = getRecordForPath(parentPath);

            checkACL(zks, parentRecord.acl, ZooDefs.Perms.CREATE,
                     request.authInfo);
            int parentCVersion = parentRecord.stat.getCversion();
            CreateMode createMode =
                CreateMode.fromFlag(createRequest.getFlags());
            if (createMode.isSequential()) {
                //如果是创建有序节点，则在path后面加上版本号
                path = path + String.format(Locale.ENGLISH, "%010d", parentCVersion);
            }
            try {
                PathUtils.validatePath(path);
            } catch(IllegalArgumentException ie) {
                LOG.info("Invalid path " + path + " with session 0x" +
                         Long.toHexString(request.sessionId));
                throw new KeeperException.BadArgumentsException(path);
            }
            try {
                if (getRecordForPath(path) != null) {
                    throw new KeeperException.NodeExistsException(path);
                }
            } catch (KeeperException.NoNodeException e) {
                // ignore this one
            }
            boolean ephemeralParent = parentRecord.stat.getEphemeralOwner() != 0;
            //临时节点不能创建子节点
            if (ephemeralParent) {
                throw new KeeperException.NoChildrenForEphemeralsException(path);
            }
            //版本号+1
            int newCversion = parentRecord.stat.getCversion()+1;
            request.txn = new CreateTxn(path, createRequest.getData(),
                                        listACL,
                                        createMode.isEphemeral(), newCversion);
            StatPersisted s = new StatPersisted();
            if (createMode.isEphemeral()) {
                s.setEphemeralOwner(request.sessionId);
            }
            parentRecord = parentRecord.duplicate(request.hdr.getZxid());
            parentRecord.childCount++;
            //版本号newCversion+1后重新赋值，下一次创建顺序节点，就可以拼上不同的序号
            parentRecord.stat.setCversion(newCversion);
			//维护节点数据到outstandingChanges队列
            addChangeRecord(parentRecord);
            //维护path和节点数据到outstandingChangesForPath结构中
            addChangeRecord(new ChangeRecord(request.hdr.getZxid(), path, s,
                                             0, listACL));
            break;
        case OpCode.delete:
            zks.sessionTracker.checkSession(request.sessionId, request.getOwner());
            DeleteRequest deleteRequest = (DeleteRequest)record;
            if(deserialize)
                ByteBufferInputStream.byteBuffer2Record(request.request, deleteRequest);
            path = deleteRequest.getPath();
            lastSlash = path.lastIndexOf('/');
            if (lastSlash == -1 || path.indexOf('\0') != -1
                || zks.getZKDatabase().isSpecialPath(path)) {
                throw new KeeperException.BadArgumentsException(path);
            }
            parentPath = path.substring(0, lastSlash);
            parentRecord = getRecordForPath(parentPath);
            ChangeRecord nodeRecord = getRecordForPath(path);
            checkACL(zks, parentRecord.acl, ZooDefs.Perms.DELETE,
                     request.authInfo);
            int version = deleteRequest.getVersion();
            if (version != -1 && nodeRecord.stat.getVersion() != version) {
                throw new KeeperException.BadVersionException(path);
            }
            if (nodeRecord.childCount > 0) {
                throw new KeeperException.NotEmptyException(path);
            }
            request.txn = new DeleteTxn(path);
            parentRecord = parentRecord.duplicate(request.hdr.getZxid());
            parentRecord.childCount--;
            addChangeRecord(parentRecord);
            addChangeRecord(new ChangeRecord(request.hdr.getZxid(), path,
                                             null, -1, null));
            break;
        case OpCode.setData:
            zks.sessionTracker.checkSession(request.sessionId, request.getOwner());
            SetDataRequest setDataRequest = (SetDataRequest)record;
            if(deserialize)
                ByteBufferInputStream.byteBuffer2Record(request.request, setDataRequest);
            path = setDataRequest.getPath();
            nodeRecord = getRecordForPath(path);
            checkACL(zks, nodeRecord.acl, ZooDefs.Perms.WRITE,
                     request.authInfo);
            version = setDataRequest.getVersion();
            int currentVersion = nodeRecord.stat.getVersion();
            if (version != -1 && version != currentVersion) {
                throw new KeeperException.BadVersionException(path);
            }
            version = currentVersion + 1;
            request.txn = new SetDataTxn(path, setDataRequest.getData(), version);
            nodeRecord = nodeRecord.duplicate(request.hdr.getZxid());
            nodeRecord.stat.setVersion(version);
            addChangeRecord(nodeRecord);
            break;
        case OpCode.setACL:
            zks.sessionTracker.checkSession(request.sessionId, request.getOwner());
            SetACLRequest setAclRequest = (SetACLRequest)record;
            if(deserialize)
                ByteBufferInputStream.byteBuffer2Record(request.request, setAclRequest);
            path = setAclRequest.getPath();
            listACL = removeDuplicates(setAclRequest.getAcl());
            if (!fixupACL(request.authInfo, listACL)) {
                throw new KeeperException.InvalidACLException(path);
            }
            nodeRecord = getRecordForPath(path);
            checkACL(zks, nodeRecord.acl, ZooDefs.Perms.ADMIN,
                     request.authInfo);
            version = setAclRequest.getVersion();
            currentVersion = nodeRecord.stat.getAversion();
            if (version != -1 && version != currentVersion) {
                throw new KeeperException.BadVersionException(path);
            }
            version = currentVersion + 1;
            request.txn = new SetACLTxn(path, listACL, version);
            nodeRecord = nodeRecord.duplicate(request.hdr.getZxid());
            nodeRecord.stat.setAversion(version);
            addChangeRecord(nodeRecord);
            break;
        case OpCode.createSession:
            request.request.rewind();
            int to = request.request.getInt();
            request.txn = new CreateSessionTxn(to);
            request.request.rewind();
            zks.sessionTracker.addSession(request.sessionId, to);
            zks.setOwner(request.sessionId, request.getOwner());
            break;
        case OpCode.closeSession:
            // We don't want to do this check since the session expiration thread
            // queues up this operation without being the session owner.
            // this request is the last of the session so it should be ok
            //zks.sessionTracker.checkSession(request.sessionId, request.getOwner());
            HashSet<String> es = zks.getZKDatabase()
                .getEphemerals(request.sessionId);
            synchronized (zks.outstandingChanges) {
                for (ChangeRecord c : zks.outstandingChanges) {
                    if (c.stat == null) {
                        // Doing a delete
                        es.remove(c.path);
                    } else if (c.stat.getEphemeralOwner() == request.sessionId) {
                        es.add(c.path);
                    }
                }
                for (String path2Delete : es) {
                    addChangeRecord(new ChangeRecord(request.hdr.getZxid(),
                                                     path2Delete, null, 0, null));
                }

                zks.sessionTracker.setSessionClosing(request.sessionId);
            }

            LOG.info("Processed session termination for sessionid: 0x"
                     + Long.toHexString(request.sessionId));
            break;
        case OpCode.check:
            zks.sessionTracker.checkSession(request.sessionId, request.getOwner());
            CheckVersionRequest checkVersionRequest = (CheckVersionRequest)record;
            if(deserialize)
                ByteBufferInputStream.byteBuffer2Record(request.request, checkVersionRequest);
            path = checkVersionRequest.getPath();
            nodeRecord = getRecordForPath(path);
            checkACL(zks, nodeRecord.acl, ZooDefs.Perms.READ,
                     request.authInfo);
            version = checkVersionRequest.getVersion();
            currentVersion = nodeRecord.stat.getVersion();
            if (version != -1 && version != currentVersion) {
                throw new KeeperException.BadVersionException(path);
            }
            version = currentVersion + 1;
            request.txn = new CheckVersionTxn(path, version);
            break;
    }
}
```

## ProposalRequestProcessor处理proposal消息

核心流程

1. 先将消息交给下一个CommitProcessor处理器处理，阻塞等待commit阶段的完成
2. 然后封装事物消息，发送PROPOSAL的消息类型给所有的follower
3. 发送给其他follower的消息放入服务端处理每个客户端对应的bio线程所在的队列即可
4. 同时发送给其他follower的消息会存储到outstandingProposals中，后续判断是否接收到大多数follower返回的ack
5. 最后执行leader本身的Proposal阶段，将数据刷盘并返回给自己ACK

```java
/**
 * This RequestProcessor simply forwards requests to an AckRequestProcessor and
 * SyncRequestProcessor.
 */
public class ProposalRequestProcessor implements RequestProcessor {
    private static final Logger LOG =
        LoggerFactory.getLogger(ProposalRequestProcessor.class);

    LeaderZooKeeperServer zks;
    
    RequestProcessor nextProcessor;

    SyncRequestProcessor syncProcessor;

    public ProposalRequestProcessor(LeaderZooKeeperServer zks,
            RequestProcessor nextProcessor) {
        this.zks = zks;
        this.nextProcessor = nextProcessor;
        AckRequestProcessor ackProcessor = new AckRequestProcessor(zks.getLeader());
        syncProcessor = new SyncRequestProcessor(zks, ackProcessor);
    }
    
    /**
     * initialize this processor
     */
    public void initialize() {
        syncProcessor.start();
    }
    
    public void processRequest(Request request) throws RequestProcessorException {
        // LOG.warn("Ack>>> cxid = " + request.cxid + " type = " +
        // request.type + " id = " + request.sessionId);
        // request.addRQRec(">prop");
                
        
        /* In the following IF-THEN-ELSE block, we process syncs on the leader. 
         * If the sync is coming from a follower, then the follower
         * handler adds it to syncHandler. Otherwise, if it is a client of
         * the leader that issued the sync command, then syncHandler won't 
         * contain the handler. In this case, we add it to syncHandler, and 
         * call processRequest on the next processor.
         */

        //同步请求
        if(request instanceof LearnerSyncRequest){
            zks.getLeader().processSync((LearnerSyncRequest)request);
        } else {
            //下一个处理器会阻塞等待commit操作的完成。但是这里不会被阻塞
            //完成commit后，继续交给下下个处理器
            nextProcessor.processRequest(request);
            if (request.hdr != null) {
                // We need to sync and get consensus on any transactions
                try {
                    //创建事物消息并发送给所有follower
                    zks.getLeader().propose(request);
                } catch (XidRolloverException e) {
                    throw new RequestProcessorException(e.getMessage(), e);
                }
                //将数据追加到本地磁盘
                syncProcessor.processRequest(request);
            }
        }
    }

    public void shutdown() {
        LOG.info("Shutting down");
        nextProcessor.shutdown();
        syncProcessor.shutdown();
    }

}
```

调用的是leader的propose方法将消息发送给所有follower。将消息封装为Proposal，放入到outstandingProposals队列中，后续可以判断ack是否完成

```java
//保存leader发送给follower的消息
ConcurrentMap<Long, Proposal> outstandingProposals = new ConcurrentHashMap<Long, Proposal>();

/**
* create a proposal and send it out to all the members
* 
* @param request
* @return the proposal that is queued to send to all the members
*
* 创建事物消息并发送给所有follower
*/
public Proposal propose(Request request) throws XidRolloverException {
    /**
         * Address the rollover issue. All lower 32bits set indicate a new leader
         * election. Force a re-election instead. See ZOOKEEPER-1277
         */
    if ((request.zxid & 0xffffffffL) == 0xffffffffL) {
        String msg =
            "zxid lower 32 bits have rolled over, forcing re-election, and therefore new epoch start";
        shutdown(msg);
        throw new XidRolloverException(msg);
    }

    ByteArrayOutputStream baos = new ByteArrayOutputStream();
    BinaryOutputArchive boa = BinaryOutputArchive.getArchive(baos);
    try {
        request.hdr.serialize(boa, "hdr");
        if (request.txn != null) {
            request.txn.serialize(boa, "txn");
        }
        baos.close();
    } catch (IOException e) {
        LOG.warn("This really should be impossible", e);
    }
    QuorumPacket pp = new QuorumPacket(Leader.PROPOSAL, request.zxid, 
                                       baos.toByteArray(), null);

    Proposal p = new Proposal();
    p.packet = pp;
    p.request = request;
    synchronized (this) {
        if (LOG.isDebugEnabled()) {
            LOG.debug("Proposing:: " + request);
        }

        lastProposed = p.packet.getZxid();
        //将发送出去的proposal消息放入到outstandingProposals队列，后续接收到大多数ack响应后，从这里来获取对应的消息
        outstandingProposals.put(lastProposed, p);
        //发送给所有的follower
        sendPacket(pp);
    }
    return p;
}
```

发送数据给其他follower，只需要将消息加入到每个客户端处理线程LearnerHandler所处理的队列中即可

```java
void sendPacket(QuorumPacket qp) {
    synchronized (forwardingFollowers) {
        for (LearnerHandler f : forwardingFollowers) {
            //放入队列
            f.queuePacket(qp);
        }
    }
}
```

## SyncRequestProcessor追加数据到磁盘文件

同follower处理链路一致，将数据刷盘，执行完flush操作后会执行下一个链路AckRequestProcessor处理器

## AckRequestProcessor处理ack响应

大多数的ACK包括了leader节点本身，因此执行完刷盘（Proposal)操作后，需要返回ACK

```java
/**
 * This is a very simple RequestProcessor that simply forwards a request from a
 * previous stage to the leader as an ACK.
 */
class AckRequestProcessor implements RequestProcessor {
    private static final Logger LOG = LoggerFactory.getLogger(AckRequestProcessor.class);
    Leader leader;

    AckRequestProcessor(Leader leader) {
        this.leader = leader;
    }

    /**
     * Forward the request as an ACK to the leader
     */
    public void processRequest(Request request) {
        QuorumPeer self = leader.self;
        if(self != null)
            //leader返回自己ack，然后判断是否满足大多数
            leader.processAck(self.getId(), request.zxid, null);
        else
            LOG.error("Null QuorumPeer");
    }

    public void shutdown() {
        // XXX No need to do anything
    }
}
```

### leader接收ACK请求处理

当leader接收到follower发送的ack响应或者自己直接完成数据追加后给自己响应ack，需要判断是否接收到Proposal的大多数ack响应，如果满足则

1. 从outstandingProposals队列移除（leader发送事物消息给follower的时候放入的队列)
2. 加入到toBeApplied队列，leader有ToBeAppliedRequestProcessor处理器处理
3. 给所有follower发送COMMIT类型消息
4. 执行leader的commit方法（在CommitProcessor处理器）

```java
case Leader.ACK:
if (this.learnerType == LearnerType.OBSERVER) {
    if (LOG.isDebugEnabled()) {
        LOG.debug("Received ACK from Observer  " + this.sid);
    }
}
leader.processAck(this.sid, qp.getZxid(), sock.getLocalSocketAddress());
break;
```

```java
/**
 * Keep a count of acks that are received by the leader for a particular
 * proposal
 * 
 * @param zxid
 *                the zxid of the proposal sent out
 * @param followerAddr
 *
 *
 * 之前leader发送给其他follower的消息放在leader全局的outstandingProposals map中保存
 * 这里接收到follower返回的ack后，进行计算大多数响应逻辑
 */
synchronized public void processAck(long sid, long zxid, SocketAddress followerAddr) {
    if (LOG.isTraceEnabled()) {
        LOG.trace("Ack zxid: 0x{}", Long.toHexString(zxid));
        for (Proposal p : outstandingProposals.values()) {
            long packetZxid = p.packet.getZxid();
            LOG.trace("outstanding proposal: 0x{}",
                      Long.toHexString(packetZxid));
        }
        LOG.trace("outstanding proposals all");
    }

    //outstandingProposals保存了之前leader发送给follower的消息
    if (outstandingProposals.size() == 0) {
        if (LOG.isDebugEnabled()) {
            LOG.debug("outstanding is 0");
        }
        return;
    }
    if (lastCommitted >= zxid) {
        if (LOG.isDebugEnabled()) {
            LOG.debug("proposal has already been committed, pzxid: 0x{} zxid: 0x{}",
                      Long.toHexString(lastCommitted), Long.toHexString(zxid));
        }
        // The proposal has already been committed
        return;
    }
    //获取发送给follower的Proposal消息
    Proposal p = outstandingProposals.get(zxid);
    if (p == null) {
        LOG.warn("Trying to commit future proposal: zxid 0x{} from {}",
                 Long.toHexString(zxid), followerAddr);
        return;
    }

    //通过消息本身ackSet来存储哪个follower发送过ack
    //这里记录哪个follower接收到了消息，返回了ack
    p.ackSet.add(sid);
    if (LOG.isDebugEnabled()) {
        LOG.debug("Count for zxid: 0x{} is {}",
                  Long.toHexString(zxid), p.ackSet.size());
    }
    //验证接收到ack的follower数量是否超过了follower的一半（大多数）
    if (self.getQuorumVerifier().containsQuorum(p.ackSet)){             
        if (zxid != lastCommitted+1) {
            LOG.warn("Commiting zxid 0x{} from {} not first!",
                     Long.toHexString(zxid), followerAddr);
            LOG.warn("First is 0x{}", Long.toHexString(lastCommitted + 1));
        }
        //接收到大多数ack后从outstandingProposals中移除，加入到toBeApplied队列
        outstandingProposals.remove(zxid);
        if (p.request != null) {
            //加入到toBeApplied队列
            toBeApplied.add(p);
        }
        // We don't commit the new leader proposal
        if ((zxid & 0xffffffffL) != 0) {
            if (p.request == null) {
                LOG.warn("Going to commmit null request for proposal: {}", p);
            }
            //开始commit消息，将消息发送给其他所有follower
            commit(zxid);

            //发送INFORM到Observer
            inform(p);
			
            //执行commitProcessor处理器的commit方法
            zk.commitProcessor.commit(p.request);
            if(pendingSyncs.containsKey(zxid)){
                for(LearnerSyncRequest r: pendingSyncs.remove(zxid)) {
                    sendSync(r);
                }
            }
            return;
        } else {
            lastCommitted = zxid;
            LOG.info("Have quorum of supporters; starting up and setting last processed zxid: 0x{}",
                     Long.toHexString(zk.getZxid()));
            //这里很关键，如果是新的leader消息，需要开启session过期线程以及设置好消息处理链路
            zk.startup();
            //更新内存元数据zxid
            zk.getZKDatabase().setlastProcessedZxid(zk.getZxid());
        }
    }
}
```

发送COMMIT类型消息给所有follower

```java
/**
 * Create a commit packet and send it to all the members of the quorum
 * 
 * @param zxid
 */
public void commit(long zxid) {
    synchronized(this){
        lastCommitted = zxid;
    }
    QuorumPacket qp = new QuorumPacket(Leader.COMMIT, zxid, null, null);
    //再次发送COMMIT消息给follower
    sendPacket(qp);
}
```

## CommitProcessor处理消息commit

在ProposalRequestProcessor处理器处理完消息后，交给该处理器来完成commit操作，在此之前会阻塞。等待Leader发送完proposal消息，并接收到大多数节点返回ack，给所有节点发送commit消息后，会执行Leader自己的commit操作，调用CommitProcessor处理器的commit方法。主要是唤醒之前阻塞在之前等待commit完成的操作，并交给下一个处理器去处理

```java
/**
 * This RequestProcessor matches the incoming committed requests with the
 * locally submitted requests. The trick is that locally submitted requests that
 * change the state of the system will come back as incoming committed requests,
 * so we need to match them up.
 */
public class CommitProcessor extends Thread implements RequestProcessor {
    private static final Logger LOG = LoggerFactory.getLogger(CommitProcessor.class);

    /**
     * Requests that we are holding until the commit comes in.
     * 接收到的消息，待执行commit操作
     */
    LinkedList<Request> queuedRequests = new LinkedList<Request>();

    /**
     * Requests that have been committed.
     * 完成commit的请求
     */
    LinkedList<Request> committedRequests = new LinkedList<Request>();

    RequestProcessor nextProcessor;
    //需要移交给下一个处理器处理的消息队列
    ArrayList<Request> toProcess = new ArrayList<Request>();

    /**
     * This flag indicates whether we need to wait for a response to come back from the
     * leader or we just let the sync operation flow through like a read. The flag will
     * be true if the CommitProcessor is in a Leader pipeline.
     */
    boolean matchSyncs;

    public CommitProcessor(RequestProcessor nextProcessor, String id, boolean matchSyncs) {
        super("CommitProcessor:" + id);
        this.nextProcessor = nextProcessor;
        this.matchSyncs = matchSyncs;
    }

    volatile boolean finished = false;

    @Override
    public void run() {
        try {
            Request nextPending = null;            
            while (!finished) {
                int len = toProcess.size();
                for (int i = 0; i < len; i++) {
                    //消息处理完commit后，交给下一个处理器处理
                    nextProcessor.processRequest(toProcess.get(i));
                }
                toProcess.clear();
                synchronized (this) {

                    //初始状态都在这里阻塞

                    //leader收到消息后，放入到queuedRequests队列中，但是committedRequests仍然是空，所以会继续阻塞
                    //需要等待消息的commit阶段完成，才会从这里返回

                    if ((queuedRequests.size() == 0 || nextPending != null)
                            && committedRequests.size() == 0) {
                        wait();
                        continue;
                    }
                    // First check and see if the commit came in for the pending
                    // request
                    //当接收到commit通知时
                    if ((queuedRequests.size() == 0 || nextPending != null)
                            && committedRequests.size() > 0) {

                        //从committedRequests队列移除消息
                        Request r = committedRequests.remove();
                        /*
                         * We match with nextPending so that we can move to the
                         * next request when it is committed. We also want to
                         * use nextPending because it has the cnxn member set
                         * properly.
                         */
                        if (nextPending != null
                                && nextPending.sessionId == r.sessionId
                                && nextPending.cxid == r.cxid) {
                            // we want to send our version of the request.
                            // the pointer to the connection in the request
                            nextPending.hdr = r.hdr;
                            nextPending.txn = r.txn;
                            nextPending.zxid = r.zxid;
                            //加入到toProcess队列让下一个处理器处理
                            //nextPending消息接收到commit完成的通知，交给下一个处理器处理
                            toProcess.add(nextPending);
                            nextPending = null;
                        } else {
                            // this request came from someone else so just
                            // send the commit packet
                            toProcess.add(r);
                        }
                    }
                }

                // We haven't matched the pending requests, so go back to
                // waiting
                if (nextPending != null) {
                    continue;
                }

                synchronized (this) {
                    // Process the next requests in the queuedRequests
                    //处理queuedRequests队列数据
                    while (nextPending == null && queuedRequests.size() > 0) {
                        //将请求从queuedRequests队列移除
                        Request request = queuedRequests.remove();
                        switch (request.type) {
                        case OpCode.create:
                        case OpCode.delete:
                        case OpCode.setData:
                        case OpCode.multi:
                        case OpCode.setACL:
                        case OpCode.createSession:
                        case OpCode.closeSession:
                            //将请求赋值给nextPending
                            nextPending = request;
                            break;
                        case OpCode.sync:
                            if (matchSyncs) {
                                nextPending = request;
                            } else {
                                toProcess.add(request);
                            }
                            break;
                        default:
                            toProcess.add(request);
                        }
                    }
                }
            }
        } catch (InterruptedException e) {
            LOG.warn("Interrupted exception while waiting", e);
        } catch (Throwable e) {
            LOG.error("Unexpected exception causing CommitProcessor to exit", e);
        }
        LOG.info("CommitProcessor exited loop!");
    }

    //在leader接收到大多数follower返回的ack后，会执行该commit方法
    synchronized public void commit(Request request) {
        if (!finished) {
            if (request == null) {
                LOG.warn("Committed a null!",
                         new Exception("committing a null! "));
                return;
            }
            if (LOG.isDebugEnabled()) {
                LOG.debug("Committing request:: " + request);
            }
            committedRequests.add(request);
            notifyAll();
        }
    }

    synchronized public void processRequest(Request request) {
        // request.addRQRec(">commit");
        if (LOG.isDebugEnabled()) {
            LOG.debug("Processing request:: " + request);
        }
        
        if (!finished) {
            //有请求需要处理，唤醒当前线程
            queuedRequests.add(request);
            notifyAll();
        }
    }

    public void shutdown() {
        LOG.info("Shutting down");
        synchronized (this) {
            finished = true;
            queuedRequests.clear();
            notifyAll();
        }
        if (nextProcessor != null) {
            nextProcessor.shutdown();
        }
    }

}
```

## ToBeAppliedRequestProcessor维护toBeApplied列表处理器

```java
static class ToBeAppliedRequestProcessor implements RequestProcessor {
    private RequestProcessor next;

    private ConcurrentLinkedQueue<Proposal> toBeApplied;

    /**
     * This request processor simply maintains the toBeApplied list. For
     * this to work next must be a FinalRequestProcessor and
     * FinalRequestProcessor.processRequest MUST process the request
     * synchronously!
     * 
     * @param next
     * a reference to the FinalRequestProcessor
     */
    ToBeAppliedRequestProcessor(RequestProcessor next,
                                ConcurrentLinkedQueue<Proposal> toBeApplied) {
        if (!(next instanceof FinalRequestProcessor)) {
            throw new RuntimeException(ToBeAppliedRequestProcessor.class
                                       .getName()
                                       + " must be connected to "
                                       + FinalRequestProcessor.class.getName()
                                       + " not "
                                       + next.getClass().getName());
        }
        this.toBeApplied = toBeApplied;
        this.next = next;
    }

    /*
     * (non-Javadoc)
     * 
     * @see org.apache.zookeeper.server.RequestProcessor#processRequest(org.apache.zookeeper.server.Request)
     */
    //该处理器没有做什么核心工作，直接交给下一个process处理
    //从toBeApplied队列移除
    public void processRequest(Request request) throws RequestProcessorException {
        // request.addRQRec(">tobe");
        next.processRequest(request);
        Proposal p = toBeApplied.peek();
        if (p != null && p.request != null
            && p.request.zxid == request.zxid) {
            toBeApplied.remove();
        }
    }

    public void shutdown() {
        LOG.info("Shutting down");
        next.shutdown();
    }
}
```

## FinalRequestProcessor处理内存数据库

1. 处理COMMIT完成的请求，作用于内存数据可以
2. 构造响应数据，通过客户端NIO连接返回

```java
/**
 * This Request processor actually applies any transaction associated with a
 * request and services any queries. It is always at the end of a
 * RequestProcessor chain (hence the name), so it does not have a nextProcessor
 * member.
 *
 * This RequestProcessor counts on ZooKeeperServer to populate the
 * outstandingRequests member of ZooKeeperServer.
 *
 * 返回给客户端响应数据
 */
public class FinalRequestProcessor implements RequestProcessor {
    private static final Logger LOG = LoggerFactory.getLogger(FinalRequestProcessor.class);

    ZooKeeperServer zks;

    public FinalRequestProcessor(ZooKeeperServer zks) {
        this.zks = zks;
    }

    public void processRequest(Request request) {
        if (LOG.isDebugEnabled()) {
            LOG.debug("Processing request:: " + request);
        }
        // request.addRQRec(">final");
        long traceMask = ZooTrace.CLIENT_REQUEST_TRACE_MASK;
        if (request.type == OpCode.ping) {
            traceMask = ZooTrace.SERVER_PING_TRACE_MASK;
        }
        if (LOG.isTraceEnabled()) {
            ZooTrace.logRequest(LOG, traceMask, 'E', request, "");
        }
        ProcessTxnResult rc = null;
        synchronized (zks.outstandingChanges) {
            while (!zks.outstandingChanges.isEmpty()
                    && zks.outstandingChanges.get(0).zxid <= request.zxid) {
                ChangeRecord cr = zks.outstandingChanges.remove(0);
                if (cr.zxid < request.zxid) {
                    LOG.warn("Zxid outstanding "
                            + cr.zxid
                            + " is less than current " + request.zxid);
                }
                if (zks.outstandingChangesForPath.get(cr.path) == cr) {
                    zks.outstandingChangesForPath.remove(cr.path);
                }
            }
            if (request.hdr != null) {
               TxnHeader hdr = request.hdr;
               Record txn = request.txn;
			   //处理内存中的数据
               rc = zks.processTxn(hdr, txn);
            }
            // do not add non quorum packets to the queue.
            //把request加入到ZKDatabase.committedLog队列中，这个队列主要是为了快速和follower同步而保留的
            if (Request.isQuorum(request.type)) {
                zks.getZKDatabase().addCommittedProposal(request);
            }
        }

        if (request.hdr != null && request.hdr.getType() == OpCode.closeSession) {
            ServerCnxnFactory scxn = zks.getServerCnxnFactory();
            // this might be possible since
            // we might just be playing diffs from the leader
            if (scxn != null && request.cnxn == null) {
                // calling this if we have the cnxn results in the client's
                // close session response being lost - we've already closed
                // the session/socket here before we can send the closeSession
                // in the switch block below
                scxn.closeSession(request.sessionId);
                return;
            }
        }

        if (request.cnxn == null) {
            return;
        }
        ServerCnxn cnxn = request.cnxn;

        String lastOp = "NA";
        zks.decInProcess();
        Code err = Code.OK;
        Record rsp = null;
        boolean closeSession = false;
        try {
            if (request.hdr != null && request.hdr.getType() == OpCode.error) {
                throw KeeperException.create(KeeperException.Code.get((
                        (ErrorTxn) request.txn).getErr()));
            }

            KeeperException ke = request.getException();
            if (ke != null && request.type != OpCode.multi) {
                throw ke;
            }

            if (LOG.isDebugEnabled()) {
                LOG.debug("{}",request);
            }
            switch (request.type) {
            case OpCode.ping: {
                zks.serverStats().updateLatency(request.createTime);

                lastOp = "PING";
                cnxn.updateStatsForResponse(request.cxid, request.zxid, lastOp,
                        request.createTime, System.currentTimeMillis());

                cnxn.sendResponse(new ReplyHeader(-2,
                        zks.getZKDatabase().getDataTreeLastProcessedZxid(), 0), null, "response");
                return;
            }
            case OpCode.createSession: {
                zks.serverStats().updateLatency(request.createTime);

                lastOp = "SESS";
                cnxn.updateStatsForResponse(request.cxid, request.zxid, lastOp,
                        request.createTime, System.currentTimeMillis());

                zks.finishSessionInit(request.cnxn, true);
                return;
            }
            case OpCode.multi: {
                lastOp = "MULT";
                rsp = new MultiResponse() ;

                for (ProcessTxnResult subTxnResult : rc.multiResult) {

                    OpResult subResult ;

                    switch (subTxnResult.type) {
                        case OpCode.check:
                            subResult = new CheckResult();
                            break;
                        case OpCode.create:
                            subResult = new CreateResult(subTxnResult.path);
                            break;
                        case OpCode.delete:
                            subResult = new DeleteResult();
                            break;
                        case OpCode.setData:
                            subResult = new SetDataResult(subTxnResult.stat);
                            break;
                        case OpCode.error:
                            subResult = new ErrorResult(subTxnResult.err) ;
                            break;
                        default:
                            throw new IOException("Invalid type of op");
                    }

                    ((MultiResponse)rsp).add(subResult);
                }

                break;
            }
            case OpCode.create: {
                lastOp = "CREA";
                rsp = new CreateResponse(rc.path);
                err = Code.get(rc.err);
                break;
            }
            case OpCode.delete: {
                lastOp = "DELE";
                err = Code.get(rc.err);
                break;
            }
            case OpCode.setData: {
                lastOp = "SETD";
                rsp = new SetDataResponse(rc.stat);
                err = Code.get(rc.err);
                break;
            }
            case OpCode.setACL: {
                lastOp = "SETA";
                rsp = new SetACLResponse(rc.stat);
                err = Code.get(rc.err);
                break;
            }
            case OpCode.closeSession: {
                lastOp = "CLOS";
                closeSession = true;
                err = Code.get(rc.err);
                break;
            }
            case OpCode.sync: {
                lastOp = "SYNC";
                SyncRequest syncRequest = new SyncRequest();
                ByteBufferInputStream.byteBuffer2Record(request.request,
                        syncRequest);
                rsp = new SyncResponse(syncRequest.getPath());
                break;
            }
            case OpCode.check: {
                lastOp = "CHEC";
                rsp = new SetDataResponse(rc.stat);
                err = Code.get(rc.err);
                break;
            }
            case OpCode.exists: {
                lastOp = "EXIS";
                // TODO we need to figure out the security requirement for this!
                ExistsRequest existsRequest = new ExistsRequest();
                ByteBufferInputStream.byteBuffer2Record(request.request,
                        existsRequest);
                String path = existsRequest.getPath();
                if (path.indexOf('\0') != -1) {
                    throw new KeeperException.BadArgumentsException();
                }
                Stat stat = zks.getZKDatabase().statNode(path, existsRequest
                        .getWatch() ? cnxn : null);
                rsp = new ExistsResponse(stat);
                break;
            }
            case OpCode.getData: {
                lastOp = "GETD";
                GetDataRequest getDataRequest = new GetDataRequest();
                ByteBufferInputStream.byteBuffer2Record(request.request,
                        getDataRequest);
                DataNode n = zks.getZKDatabase().getNode(getDataRequest.getPath());
                if (n == null) {
                    throw new KeeperException.NoNodeException();
                }
                Long aclL;
                synchronized(n) {
                    aclL = n.acl;
                }
                PrepRequestProcessor.checkACL(zks, zks.getZKDatabase().convertLong(aclL),
                        ZooDefs.Perms.READ,
                        request.authInfo);
                Stat stat = new Stat();
                byte b[] = zks.getZKDatabase().getData(getDataRequest.getPath(), stat,
                        getDataRequest.getWatch() ? cnxn : null);
                rsp = new GetDataResponse(b, stat);
                break;
            }
            case OpCode.setWatches: {
                lastOp = "SETW";
                SetWatches setWatches = new SetWatches();
                // XXX We really should NOT need this!!!!
                request.request.rewind();
                ByteBufferInputStream.byteBuffer2Record(request.request, setWatches);
                long relativeZxid = setWatches.getRelativeZxid();
                zks.getZKDatabase().setWatches(relativeZxid, 
                        setWatches.getDataWatches(), 
                        setWatches.getExistWatches(),
                        setWatches.getChildWatches(), cnxn);
                break;
            }
            case OpCode.getACL: {
                lastOp = "GETA";
                GetACLRequest getACLRequest = new GetACLRequest();
                ByteBufferInputStream.byteBuffer2Record(request.request,
                        getACLRequest);
                Stat stat = new Stat();
                List<ACL> acl = 
                    zks.getZKDatabase().getACL(getACLRequest.getPath(), stat);
                rsp = new GetACLResponse(acl, stat);
                break;
            }
            case OpCode.getChildren: {
                lastOp = "GETC";
                GetChildrenRequest getChildrenRequest = new GetChildrenRequest();
                ByteBufferInputStream.byteBuffer2Record(request.request,
                        getChildrenRequest);
                DataNode n = zks.getZKDatabase().getNode(getChildrenRequest.getPath());
                if (n == null) {
                    throw new KeeperException.NoNodeException();
                }
                Long aclG;
                synchronized(n) {
                    aclG = n.acl;
                    
                }
                PrepRequestProcessor.checkACL(zks, zks.getZKDatabase().convertLong(aclG), 
                        ZooDefs.Perms.READ,
                        request.authInfo);
                List<String> children = zks.getZKDatabase().getChildren(
                        getChildrenRequest.getPath(), null, getChildrenRequest
                                .getWatch() ? cnxn : null);
                rsp = new GetChildrenResponse(children);
                break;
            }
            case OpCode.getChildren2: {
                lastOp = "GETC";
                GetChildren2Request getChildren2Request = new GetChildren2Request();
                ByteBufferInputStream.byteBuffer2Record(request.request,
                        getChildren2Request);
                Stat stat = new Stat();
                DataNode n = zks.getZKDatabase().getNode(getChildren2Request.getPath());
                if (n == null) {
                    throw new KeeperException.NoNodeException();
                }
                Long aclG;
                synchronized(n) {
                    aclG = n.acl;
                }
                PrepRequestProcessor.checkACL(zks, zks.getZKDatabase().convertLong(aclG), 
                        ZooDefs.Perms.READ,
                        request.authInfo);
                List<String> children = zks.getZKDatabase().getChildren(
                        getChildren2Request.getPath(), stat, getChildren2Request
                                .getWatch() ? cnxn : null);
                rsp = new GetChildren2Response(children, stat);
                break;
            }
            }
        } catch (SessionMovedException e) {
            // session moved is a connection level error, we need to tear
            // down the connection otw ZOOKEEPER-710 might happen
            // ie client on slow follower starts to renew session, fails
            // before this completes, then tries the fast follower (leader)
            // and is successful, however the initial renew is then 
            // successfully fwd/processed by the leader and as a result
            // the client and leader disagree on where the client is most
            // recently attached (and therefore invalid SESSION MOVED generated)
            cnxn.sendCloseSession();
            return;
        } catch (KeeperException e) {
            err = e.code();
        } catch (Exception e) {
            // log at error level as we are returning a marshalling
            // error to the user
            LOG.error("Failed to process " + request, e);
            StringBuilder sb = new StringBuilder();
            ByteBuffer bb = request.request;
            bb.rewind();
            while (bb.hasRemaining()) {
                sb.append(Integer.toHexString(bb.get() & 0xff));
            }
            LOG.error("Dumping request buffer: 0x" + sb.toString());
            err = Code.MARSHALLINGERROR;
        }

        long lastZxid = zks.getZKDatabase().getDataTreeLastProcessedZxid();
        ReplyHeader hdr =
            new ReplyHeader(request.cxid, lastZxid, err.intValue());

        zks.serverStats().updateLatency(request.createTime);
        cnxn.updateStatsForResponse(request.cxid, lastZxid, lastOp,
                    request.createTime, System.currentTimeMillis());

        try {
            //通过NIO返回给客户端响应数据
            cnxn.sendResponse(hdr, rsp, "response");
            if (closeSession) {
                cnxn.sendCloseSession();
            }
        } catch (IOException e) {
            LOG.error("FIXMSG",e);
        }
    }

    public void shutdown() {
        // we are the final link in the chain
        LOG.info("shutdown of request processor complete");
    }

}
```

处理内存数据库

```java
public ProcessTxnResult processTxn(TxnHeader hdr, Record txn) {
    ProcessTxnResult rc;
    int opCode = hdr.getType();
    long sessionId = hdr.getClientId();
    rc = getZKDatabase().processTxn(hdr, txn);
    if (opCode == OpCode.createSession) {
        if (txn instanceof CreateSessionTxn) {
            CreateSessionTxn cst = (CreateSessionTxn) txn;
            sessionTracker.addSession(sessionId, cst
                                      .getTimeOut());
        } else {
            LOG.warn("*****>>>>> Got "
                     + txn.getClass() + " "
                     + txn.toString());
        }
    } else if (opCode == OpCode.closeSession) {
        sessionTracker.removeSession(sessionId);
    }
    return rc;
}
```

调用的是ZKDatabase的processTxn方法

```java
public ProcessTxnResult processTxn(TxnHeader hdr, Record txn) {
    return dataTree.processTxn(hdr, txn);
}
```

最终调用createNode方法

```java
public String createNode(String path, byte data[], List<ACL> acl,
                         long ephemeralOwner, int parentCVersion, long zxid, long time)
    throws KeeperException.NoNodeException,
KeeperException.NodeExistsException {
    int lastSlash = path.lastIndexOf('/');
    String parentName = path.substring(0, lastSlash);
    String childName = path.substring(lastSlash + 1);
    StatPersisted stat = new StatPersisted();
    stat.setCtime(time);
    stat.setMtime(time);
    stat.setCzxid(zxid);
    stat.setMzxid(zxid);
    stat.setPzxid(zxid);
    stat.setVersion(0);
    stat.setAversion(0);
    stat.setEphemeralOwner(ephemeralOwner);
    DataNode parent = nodes.get(parentName);
    //父节点不存在则抛错
    if (parent == null) {
        throw new KeeperException.NoNodeException();
    }
    //对节点的父节点加锁进行操作
    synchronized (parent) {
        Set<String> children = parent.getChildren();
        if (children != null) {
            //已经包含了path节点，再创建则抛错
            if (children.contains(childName)) {
                throw new KeeperException.NodeExistsException();
            }
        }

        if (parentCVersion == -1) {
            parentCVersion = parent.stat.getCversion();
            parentCVersion++;
        }    
        parent.stat.setCversion(parentCVersion);
        parent.stat.setPzxid(zxid);
        Long longval = convertAcls(acl);
        //构建需要创建的DataNode节点数据
        DataNode child = new DataNode(parent, data, longval, stat);
        //追加到父节点下面
        parent.addChild(childName);
        //维护path到节点的映射关系
        nodes.put(path, child);
        if (ephemeralOwner != 0) {
            //如果是临时节点，也进行维护
            HashSet<String> list = ephemerals.get(ephemeralOwner);
            if (list == null) {
                list = new HashSet<String>();
                ephemerals.put(ephemeralOwner, list);
            }
            synchronized (list) {
                list.add(path);
            }
        }
    }
    // now check if its one of the zookeeper node child
    if (parentName.startsWith(quotaZookeeper)) {
        // now check if its the limit node
        if (Quotas.limitNode.equals(childName)) {
            // this is the limit node
            // get the parent and add it to the trie
            pTrie.addPath(parentName.substring(quotaZookeeper.length()));
        }
        if (Quotas.statNode.equals(childName)) {
            updateQuotaForPath(parentName
                               .substring(quotaZookeeper.length()));
        }
    }
    // also check to update the quotas for this node
    String lastPrefix;
    if((lastPrefix = getMaxPrefixWithQuota(path)) != null) {
        // ok we have some match and need to update
        updateCount(lastPrefix, 1);
        updateBytes(lastPrefix, data == null ? 0 : data.length);
    }
    //触发2个监听器
    dataWatches.triggerWatch(path, Event.EventType.NodeCreated);
    childWatches.triggerWatch(parentName.equals("") ? "/" : parentName,
                              Event.EventType.NodeChildrenChanged);
    return path;
}
```

# Follower接收数据链式处理

核心流程：

1. 接收到客户端请求后，将阻塞在commit的完成处理器上，同时将请求通过bio发送到Leader节点
2. Leader接收到写请求后，会发送过来Proposal请求，Follower接收到后追加到磁盘文件
3. 然后Follower返回给Leader ACK响应
4. Leader接收到大多数ACK后，作用于自己内存数据库，并再次发送COMMIT给Follower
5. Follower接收到COMMIT消息后，唤醒第1步的阻塞，作用于自己的内存数据库并构造响应返回

follower接收到最终选票后，确定自己为follower执行对应的逻辑，并初始化数据处理链来处理客户端发送过来的请求

```java
@Override
protected void setupRequestProcessors() {
    RequestProcessor finalProcessor = new FinalRequestProcessor(this);
    commitProcessor = new CommitProcessor(finalProcessor,
                                          Long.toString(getServerId()), true);
    commitProcessor.start();
    firstProcessor = new FollowerRequestProcessor(this, commitProcessor);
    ((FollowerRequestProcessor) firstProcessor).start();


    syncProcessor = new SyncRequestProcessor(this,
                                             new SendAckRequestProcessor((Learner)getFollower()));
    syncProcessor.start();
}
```

Follower的第一个处理器跟Leader的不同，Follower的第一个处理器是FollowerRequestProcessor

## FollowerRequestProcessor转发客户端写处理器

将写请求发送到Leader节点上去，同时执行CommitProcessor处理器阻塞等待在commit完成的阶段

```java
public class FollowerRequestProcessor extends Thread implements
        RequestProcessor {
    private static final Logger LOG = LoggerFactory.getLogger(FollowerRequestProcessor.class);

    FollowerZooKeeperServer zks;

    RequestProcessor nextProcessor;

    LinkedBlockingQueue<Request> queuedRequests = new LinkedBlockingQueue<Request>();

    boolean finished = false;

    public FollowerRequestProcessor(FollowerZooKeeperServer zks,
            RequestProcessor nextProcessor) {
        super("FollowerRequestProcessor:" + zks.getServerId());
        this.zks = zks;
        this.nextProcessor = nextProcessor;
    }

    @Override
    public void run() {
        try {
            while (!finished) {
                Request request = queuedRequests.take();
                if (LOG.isTraceEnabled()) {
                    ZooTrace.logRequest(LOG, ZooTrace.CLIENT_REQUEST_TRACE_MASK,
                            'F', request, "");
                }
                if (request == Request.requestOfDeath) {
                    break;
                }
                // We want to queue the request to be processed before we submit
                // the request to the leader so that we are ready to receive
                // the response
                nextProcessor.processRequest(request);
                
                // We now ship the request to the leader. As with all
                // other quorum operations, sync also follows this code
                // path, but different from others, we need to keep track
                // of the sync operations this follower has pending, so we
                // add it to pendingSyncs.
                switch (request.type) {
                case OpCode.sync:
                    zks.pendingSyncs.add(request);
                    zks.getFollower().request(request);
                    break;
                    //将请求通过bio转发给Leader
                case OpCode.create:
                case OpCode.delete:
                case OpCode.setData:
                case OpCode.setACL:
                case OpCode.createSession:
                case OpCode.closeSession:
                case OpCode.multi:
                    zks.getFollower().request(request);
                    break;
                }
            }
        } catch (Exception e) {
            LOG.error("Unexpected exception causing exit", e);
        }
        LOG.info("FollowerRequestProcessor exited loop!");
    }

    /**
     * 所有请求都放入队列中，可以保证请求的一致性！！！！
     * @param request
     */
    public void processRequest(Request request) {
        if (!finished) {
            queuedRequests.add(request);
        }
    }

    public void shutdown() {
        LOG.info("Shutting down");
        finished = true;
        queuedRequests.clear();
        queuedRequests.add(Request.requestOfDeath);
        nextProcessor.shutdown();
    }

}
```

直接通过bio将请求发送到leader节点上

```java
void request(Request request) throws IOException {
    ByteArrayOutputStream baos = new ByteArrayOutputStream();
    DataOutputStream oa = new DataOutputStream(baos);
    oa.writeLong(request.sessionId);
    oa.writeInt(request.cxid);
    oa.writeInt(request.type);
    if (request.request != null) {
        request.request.rewind();
        int len = request.request.remaining();
        byte b[] = new byte[len];
        request.request.get(b);
        request.request.rewind();
        oa.write(b);
    }
    oa.close();
    QuorumPacket qp = new QuorumPacket(Leader.REQUEST, -1, baos
                                       .toByteArray(), request.authInfo);
    writePacket(qp, true);
}
```

## Follower接收到Leader发送的消息处理

```java
void followLeader() throws InterruptedException {
    ...
	while (self.isRunning()) {
        //循环获取leader发送过来的消息并处理
        readPacket(qp);
        processPacket(qp);
    }
    ...
}
```

根据不同的消息类型处理对应逻辑

```java
protected void processPacket(QuorumPacket qp) throws IOException{
    switch (qp.getType()) {
        case Leader.PING:            
            ping(qp);            
            break;
        case Leader.PROPOSAL:
            TxnHeader hdr = new TxnHeader();
            Record txn = SerializeUtils.deserializeTxn(qp.getData(), hdr);
            if (hdr.getZxid() != lastQueued + 1) {
                LOG.warn("Got zxid 0x"
                         + Long.toHexString(hdr.getZxid())
                         + " expected 0x"
                         + Long.toHexString(lastQueued + 1));
            }
            lastQueued = hdr.getZxid();
            //处理leader发送过来的PROPOSAL类型消息
            fzk.logRequest(hdr, txn);
            break;
        case Leader.COMMIT:
            //处理leader发送过来的COMMIT类型消息
            fzk.commit(qp.getZxid());
            break;
        case Leader.UPTODATE:
            LOG.error("Received an UPTODATE message after Follower started");
            break;
        case Leader.REVALIDATE:
            revalidate(qp);
            break;
        case Leader.SYNC:
            fzk.sync();
            break;
    }
}
```

## Follower处理PROPOSAL请求

Follower角色有单独的线程在不停地接收leader发送过来的请求，当到PROPOSAL消息后交给自己的处理链

Follower接收到的Proposal类型请求，将放入到pendingTxns队列中，后续执行commit操作时会依赖该队列保证数据的顺序一致性

```java
public void logRequest(TxnHeader hdr, Record txn) {
    Request request = new Request(null, hdr.getClientId(), hdr.getCxid(),
                                  hdr.getType(), null, null);
    request.hdr = hdr;
    request.txn = txn;
    request.zxid = hdr.getZxid();
    if ((request.zxid & 0xffffffffL) != 0) {
        //Follower接收到的Proposal类型请求，将放入到pendingTxns队列中，后续执行commit操作时会依赖该队列保证数据的顺序一致性
        pendingTxns.add(request);
    }
    //交给下一个处理器处理，将消息保存到事物日志文件
    syncProcessor.processRequest(request);
}
```

## SyncRequestProcessor追加数据到磁盘文件

磁盘的写入，基于写缓冲写到BufferedOutputStream封装的IO流中，当写入1000条后或者长时间没有新的数据，就执行flush刷盘操作，此时数据也是进入到os cache，在flush操作的同时主动调用force将数据真正写入磁盘文件上，才是该处理器完成的事情

**这里会保证，Proposal消息被刷入到磁盘文件后，才会继续执行下一个处理器！！**

```java
/**
 * This RequestProcessor logs requests to disk. It batches the requests to do
 * the io efficiently. The request is not passed to the next RequestProcessor
 * until its log has been synced to disk.
 *
 * 负责把写request持久化到本地磁盘，为了提高写磁盘的效率，这里使用的是缓冲写，但是会周期性（1000个request）的调用flush操作，
 * flush之后request已经确保写到磁盘了，这时会把请求传给AckRequestProcessor继续处理
 */
public class SyncRequestProcessor extends Thread implements RequestProcessor {
    private static final Logger LOG = LoggerFactory.getLogger(SyncRequestProcessor.class);
    // Zookeeper服务器
    private final ZooKeeperServer zks;
    // 请求队列，大量的事物消息都会先进入到该队列
    private final LinkedBlockingQueue<Request> queuedRequests =
        new LinkedBlockingQueue<Request>();
    // 下个处理器
    private final RequestProcessor nextProcessor;

    // 快照处理线程
    private Thread snapInProcess = null;
    // 是否在运行中
    volatile private boolean running;

    /**
     * Transactions that have been written and are waiting to be flushed to
     * disk. Basically this is the list of SyncItems whose callbacks will be
     * invoked after flush returns successfully.
     *
     */
    // 等待被刷新到磁盘的请求队列
    private final LinkedList<Request> toFlush = new LinkedList<Request>();
    // 随机数生成器
    private final Random r = new Random(System.nanoTime());
    /**
     * The number of log entries to log before starting a snapshot
     */
    // 快照个数
    private static int snapCount = ZooKeeperServer.getSnapCount();

    private final Request requestOfDeath = Request.requestOfDeath;

    public SyncRequestProcessor(ZooKeeperServer zks,
            RequestProcessor nextProcessor)
    {
        super("SyncThread:" + zks.getServerId());
        this.zks = zks;
        this.nextProcessor = nextProcessor;
        running = true;
    }

    /**
     * used by tests to check for changing
     * snapcounts
     * @param count
     */
    public static void setSnapCount(int count) {
        snapCount = count;
    }

    /**
     * used by tests to get the snapcount
     * @return the snapcount
     */
    public static int getSnapCount() {
        return snapCount;
    }

    @Override
    public void run() {
        try {
            // 写日志数量初始化为0
            int logCount = 0;

            // we do this in an attempt to ensure that not all of the servers
            // in the ensemble take a snapshot at the same time
            // 确保所有的服务器在同一时间不是使用的同一个快照
            int randRoll = r.nextInt(snapCount/2);
            while (true) {
                Request si = null;
                // 没有需要刷新到磁盘的请求
                if (toFlush.isEmpty()) {
                    // 从请求队列中取出一个请求，若队列为空也就是没有接收到事物消息则会阻塞
                    si = queuedRequests.take();
                } else {
                    //一旦toFlush中有数据需要刷新到磁盘
                    //则从queuedRequests队列中非阻塞的获取数据，继续后续的写磁盘操作
                    //当queuedRequests队列为空，也就是没有事物消息后，直接执行磁盘刷新操作
                    si = queuedRequests.poll();
                    if (si == null) {
                        flush(toFlush);
                        continue;
                    }
                }
                // 在关闭处理器之后，会添加requestOfDeath，表示关闭后不再处理请求
                if (si == requestOfDeath) {
                    break;
                }
                // 处理事物请求
                if (si != null) {
                    // track the number of records written to the log
                    // 将事物数据通过写缓冲追加到日志文件，这里只有事务性请求才会返回true
                    if (zks.getZKDatabase().append(si)) {
                        // 写入一条日志，logCount加1
                        logCount++;
                        // 满足roll the log的条件
                        // 每隔snapCount/2个request会重新生成一个snapshot并滚动一次txnlog，
                        // 同时为了避免所有的zookeeper server在同一个时间生成snapshot和滚动日志，这里会再加上一个随机数，snapCount的默认值是10w个request
                        if (logCount > (snapCount / 2 + randRoll)) {
                            randRoll = r.nextInt(snapCount/2);
                            // roll the log
                            //将当前日志文件刷新到磁盘，并重新开启一个新的日志文件
                            zks.getZKDatabase().rollLog();
                            // take a snapshot
                            //判断生成快照的线程是否正在执行
                            if (snapInProcess != null && snapInProcess.isAlive()) {
                                LOG.warn("Too busy to snap, skipping");
                            } else {
                                //开启线程负责生成快照的线程
                                snapInProcess = new Thread("Snapshot Thread") {
                                        public void run() {
                                            try {
                                                //开启新的线程生成快照文件
                                                zks.takeSnapshot();
                                            } catch(Exception e) {
                                                LOG.warn("Unexpected exception", e);
                                            }
                                        }
                                    };
                                snapInProcess.start();
                            }
                            // 重置为0
                            logCount = 0;
                        }
                    } else if (toFlush.isEmpty()) {
                        // 优化读请求，直接交给下一个处理器
                        // optimization for read heavy workloads
                        // iff this is a read, and there are no pending
                        // flushes (writes), then just pass this to the next
                        // processor
                        nextProcessor.processRequest(si);
                        if (nextProcessor instanceof Flushable) {
                            // 刷新到磁盘
                            ((Flushable)nextProcessor).flush();
                        }
                        // 跳过后续处理
                        continue;
                    }
                    //将请求添加至被刷新至磁盘的队列
                    toFlush.add(si);
                    //当有不断的事物请求发送过来，每隔1000条数据执行一次刷盘
                    if (toFlush.size() > 1000) {
                        flush(toFlush);
                    }
                }
            }
        } catch (Throwable t) {
            LOG.error("Severe unrecoverable error, exiting", t);
            running = false;
            System.exit(11);
        }
        LOG.info("SyncRequestProcessor exited!");
    }

    private void flush(LinkedList<Request> toFlush)
        throws IOException, RequestProcessorException
    {
        if (toFlush.isEmpty())
            return;

        //磁盘事物数据刷盘
        zks.getZKDatabase().commit();
        while (!toFlush.isEmpty()) {
            Request i = toFlush.remove();
            //都已经刷入磁盘后才执行下一个处理器
            nextProcessor.processRequest(i);
        }
        if (nextProcessor instanceof Flushable) {
            ((Flushable)nextProcessor).flush();
        }
    }

    public void shutdown() {
        LOG.info("Shutting down");
        queuedRequests.add(requestOfDeath);
        try {
            if(running){
                this.join();
            }
        } catch(InterruptedException e) {
            LOG.warn("Interrupted while wating for " + this + " to finish");
        }
        nextProcessor.shutdown();
    }

    public void processRequest(Request request) {
        // request.addRQRec(">sync");
        queuedRequests.add(request);
    }

}
```

### 事物数据append追加到文件

最终调用的是FileTxnLog类的append方法，底层基于BufferedOutputStream的缓冲流写入数据提升性能，并且在写入真实数据时会写入校验和避免数据被篡改

```java
/**
 * append an entry to the transaction log
 * @param hdr the header of the transaction
 * @param txn the transaction part of the entry
 * returns true iff something appended, otw false
 *
 * 追加事物日志到文件，只是进入文件流的缓存中
*/
public synchronized boolean append(TxnHeader hdr, Record txn)
    throws IOException
{
    //事物日志才返回true
    if (hdr != null) {
        if (hdr.getZxid() <= lastZxidSeen) {
            LOG.warn("Current zxid " + hdr.getZxid()
                     + " is <= " + lastZxidSeen + " for "
                     + hdr.getType());
        }
        //用消息的zxid命名为新的日志文件
        if (logStream==null) {
            if(LOG.isInfoEnabled()){
                LOG.info("Creating new log file: log." +  
                         Long.toHexString(hdr.getZxid()));
            }

            logFileWrite = new File(logDir, ("log." + 
                                             Long.toHexString(hdr.getZxid())));
            fos = new FileOutputStream(logFileWrite);
            logStream=new BufferedOutputStream(fos);
            oa = BinaryOutputArchive.getArchive(logStream);
            FileHeader fhdr = new FileHeader(TXNLOG_MAGIC,VERSION, dbId);
            fhdr.serialize(oa, "fileheader");
            // Make sure that the magic number is written before padding.
            logStream.flush();
            currentSize = fos.getChannel().position();
            streamsToFlush.add(fos);
        }
        padFile(fos);
        byte[] buf = Util.marshallTxnEntry(hdr, txn);
        if (buf == null || buf.length == 0) {
            throw new IOException("Faulty serialization for header " +
                                  "and txn");
        }
        //写校验和
        Checksum crc = makeChecksumAlgorithm();
        crc.update(buf, 0, buf.length);
        //写入checksum和数据
        //直接基于IO流写入
        oa.writeLong(crc.getValue(), "txnEntryCRC");
        Util.writeTxnBytes(oa, buf);

        return true;
    }
    return false;
}
```

### flush刷盘操作

当事物消息非常多的时候，每隔1000条消息执行一次flush输盘操作，否则长时间不满1000条消息，则当toFlush队列处理完成为空也执行flush刷盘操作

```java
private void flush(LinkedList<Request> toFlush)
    throws IOException, RequestProcessorException
{
    if (toFlush.isEmpty())
        return;

    //事物快照日志刷盘
    zks.getZKDatabase().commit();
    while (!toFlush.isEmpty()) {
        Request i = toFlush.remove();
        //都已经刷入磁盘后才执行下一个处理器
        nextProcessor.processRequest(i);
    }
    if (nextProcessor instanceof Flushable) {
        ((Flushable)nextProcessor).flush();
    }
}
```

最终调用的是FileTxnLog类的commit方法，对需要刷盘的文件调用flush方法，此时数据会进入到os cache，随后主动调用force将数据从os cache强制刷新到磁盘文件上

```java
/**
 * commit the logs. make sure that evertyhing hits the
 * disk
 */
public synchronized void commit() throws IOException {
    //将当前日志文件数据刷入磁盘
    if (logStream != null) {
        logStream.flush();
    }
    //将所有的磁盘文件刷盘
    for (FileOutputStream log : streamsToFlush) {
        //刷盘
        log.flush();
        //默认是true
        if (forceSync) {
            long startSyncNS = System.nanoTime();

            //如果有必要从os cache刷入底层磁盘中
            log.getChannel().force(false);

            long syncElapsedMS =
                TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - startSyncNS);
            if (syncElapsedMS > fsyncWarningThresholdMS) {
                LOG.warn("fsync-ing the write ahead log in "
                         + Thread.currentThread().getName()
                         + " took " + syncElapsedMS
                         + "ms which will adversely effect operation latency. "
                         + "See the ZooKeeper troubleshooting guide");
            }
        }
    }
    //清空streamsToFlush文件数据
    while (streamsToFlush.size() > 1) {
        streamsToFlush.removeFirst().close();
    }
}
```

## SendAckRequestProcessor发送ACK给leader处理器

当事物消息在当前follower写入磁盘成功后，发送ACK请求给到leader

```java
public class SendAckRequestProcessor implements RequestProcessor, Flushable {
    private static final Logger LOG = LoggerFactory.getLogger(SendAckRequestProcessor.class);
    
    Learner learner;

    SendAckRequestProcessor(Learner peer) {
        this.learner = peer;
    }

    public void processRequest(Request si) {
        if(si.type != OpCode.sync){
            //封装ack类型的数据包，发送给leader
            QuorumPacket qp = new QuorumPacket(Leader.ACK, si.hdr.getZxid(), null,
                null);
            try {
                learner.writePacket(qp, false);
            } catch (IOException e) {
                LOG.warn("Closing connection to leader, exception during packet send", e);
                try {
                    if (!learner.sock.isClosed()) {
                        learner.sock.close();
                    }
                } catch (IOException e1) {
                    // Nothing to do, we are shutting things down, so an exception here is irrelevant
                    LOG.debug("Ignoring error closing the connection", e1);
                }
            }
        }
    }
    
    public void flush() throws IOException {
        try {
            learner.writePacket(null, true);
        } catch(IOException e) {
            LOG.warn("Closing connection to leader, exception during packet send", e);
            try {
                if (!learner.sock.isClosed()) {
                    learner.sock.close();
                }
            } catch (IOException e1) {
                    // Nothing to do, we are shutting things down, so an exception here is irrelevant
                    LOG.debug("Ignoring error closing the connection", e1);
            }
        }
    }

    public void shutdown() {
        // Nothing needed
    }

}
```

## Follower处理COMMIT请求

```java
case Leader.COMMIT:
//处理leader发送过来的COMMIT类型消息
fzk.commit(qp.getZxid());
break;
```

Follower对比commit request的zxid和前面提到的pendingTxns的zxid，不一致的话Follower退出，重新跟Leader同步

```java
//事物提交
public void commit(long zxid) {
    if (pendingTxns.size() == 0) {
        LOG.warn("Committing " + Long.toHexString(zxid)
                 + " without seeing txn");
        return;
    }
    long firstElementZxid = pendingTxns.element().zxid;
    if (firstElementZxid != zxid) {
        LOG.error("Committing zxid 0x" + Long.toHexString(zxid)
                  + " but next pending txn 0x"
                  + Long.toHexString(firstElementZxid));
        System.exit(12);
    }
    Request request = pendingTxns.remove();
    commitProcessor.commit(request);
}
```

## CommitProcessor处理消息commit

同leader处理相同

## FinalRequestProcessor返回响应数据

同leader处理相同

# 如何保证数据顺序一致性

客户端发送请求到Leader，Leader会给请求赋值递增的zxid（客户端如果发送到Follower，则转发到Leader）。

Leader与Follower之间的通信通过bio，顺序发送与接收

在Follower执行commit的时候，会校验是否与Proposal时的数据zxid是一致

# 流程图

![客户端请求处理流程](D:\user\文档\cl\doc\zk\5.zab一致性协议流程.assets\客户端请求处理流程.jpg)